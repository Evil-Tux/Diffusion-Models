{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Evil-Tux/Diffusion-Models/blob/main/Diffusion_Models_One.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4aUFFj4M-F6"
      },
      "source": [
        "# Diffusion Models 101"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "340f49bd"
      },
      "outputs": [],
      "source": [
        "!pip install diffusers==0.16.1 accelerate open_clip_torch transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "nwaNOo0Xr6od"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8834f0c2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchvision\n",
        "from torchvision.transforms import Compose, Resize, ToTensor, ToPILImage\n",
        "\n",
        "from diffusers import DDPMScheduler, DDIMScheduler, DDPMPipeline, DDIMPipeline\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "def plot_images(images, n=8, axs=None):\n",
        "    if axs is None:\n",
        "        fig, axs = plt.subplots(1, n, figsize=(10, 3))\n",
        "    assert len(axs) == len(images)\n",
        "    for i, img in enumerate(images):\n",
        "        axs[i].axis('off')\n",
        "        if isinstance(img, torch.Tensor):\n",
        "            img = ToPILImage()((img/2+0.5).clamp(0, 1))\n",
        "        axs[i].imshow(img.resize((64, 64), resample=Image.NEAREST), cmap='gray_r', vmin=0, vmax=255)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6cacf4a"
      },
      "source": [
        "## The Power of Stable Diffusion\n",
        "\n",
        "Behold the mighty stable diffusion model!\n",
        "\n",
        "Open source and versatile, the stable diffusion model distinguishes itself from other image generation models by providing users access to its source code and the ability to train custom models.\n",
        "\n",
        "![](https://github.com/dvgodoy/DiffusionModels101_ODSC_Europe2023/blob/main/images/diffusion_model.png?raw=true)\n",
        "Source: [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752)\n",
        "\n",
        "The model has several moving parts, and it may look intimidating at first sight, but we'll take it apart, piece by piece, to make it more digestible.\n",
        "\n",
        "But, before start dissecting this model, let's see what it is capable of with a short example.\n",
        "\n",
        "The example below is a slightly modified version from a [notebook](https://github.com/huggingface/diffusion-models-class/blob/main/unit3/01_stable_diffusion_introduction.ipynb) in [Unit 3 of HuggingFace's Diffusion Models class](https://github.com/huggingface/diffusion-models-class/tree/main/unit3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d465e612"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from diffusers import DiffusionPipeline\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "pipe = DiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0afdde6"
      },
      "outputs": [],
      "source": [
        "# Set up a generator for reproducibility\n",
        "generator = torch.Generator(device=device).manual_seed(42)\n",
        "\n",
        "# Run the pipeline, showing some of the available arguments\n",
        "pipe_output = pipe(\n",
        "    prompt=\"impressionist painting of an autumn cityscape\", # What to generate\n",
        "    negative_prompt=\"Oversaturated, blurry, low quality\", # What NOT to generate\n",
        "    height=480, width=640,     # Specify the image size\n",
        "    guidance_scale=8,          # How strongly to follow the prompt\n",
        "    num_inference_steps=35,    # How many steps to take\n",
        "    generator=generator        # Fixed random seed\n",
        ")\n",
        "\n",
        "# View the resulting image:\n",
        "pipe_output.images[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a11a3584"
      },
      "source": [
        "Amazing, isn't it? Let's take a look at the several components that make up this pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ea54ae7f"
      },
      "outputs": [],
      "source": [
        "print(list(pipe.components.keys())) # List components"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfc5823e"
      },
      "source": [
        "We **import** libraries and modules useful for diffusion models, such as PyTorch and Hugging Face’s diffusers. We'll be using the Modified National Institute of Standards and Technology (MNIST) dataset throughout the series. Once we have our imports and dataset, it’s time to use the model.\n",
        "During training of diffusion models, **noise** transforms the image during generation. Essentially, the model begins with data, such as an image, and adds noise to it. During generation, the model uses a **scheduler** to gradually reduce the image’s noise to create a clear image. In the following sections, we’ll cover these concepts in more detail with examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62e7427f"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "Diffusion models can be large and training can be time consuming. For educational purposes, we will utilize a dataset comprising small images. This way, we can train and fine-tune models using Google Colab in a matter of minutes instead of hours, allowing us to experiment with diverse setups and configurations.\n",
        "\n",
        "In the following example, we are utilizing the frequently used [MNIST dataset](https://www.digitalocean.com/community/tutorials/mnist-dataset-in-python), which stands for Modified National Institute of Standards and Technology.\n",
        "\n",
        "Let’s resize the images from their original 28x28 pixels to 32x32 pixels and create tensors from them using Torchvision transforms directly within the dataset:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fb5bf2b6"
      },
      "outputs": [],
      "source": [
        "composed = Compose([Resize(32), ToTensor()])\n",
        "dataset = torchvision.datasets.MNIST(root=\"mnist/\", train=True, download=True, transform=composed)\n",
        "train_dataloader = DataLoader(dataset, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a77c564f"
      },
      "source": [
        "Let's use the `plot_images` helper function to visualize eight digits from the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a983fb69"
      },
      "outputs": [],
      "source": [
        "images = next(iter(train_dataloader))[0][:8]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cc50334"
      },
      "outputs": [],
      "source": [
        "plot_images(images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fd32b03"
      },
      "source": [
        "Typical MNIST data, nothing new to see here, let's move on!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce92afbe"
      },
      "source": [
        "## Noise\n",
        "\n",
        "Let's make some noise, literally! The snippet below generates eight images of pure [Gaussian noise](https://wiki.cloudfactory.com/docs/mp-wiki/augmentations/gaussian-noise):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acd742d4"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(13)\n",
        "noise = torch.randn_like(images)\n",
        "noise.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6420a27"
      },
      "outputs": [],
      "source": [
        "plot_images(noise)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2861526b"
      },
      "source": [
        "Believe it or not, diffusion models are able to transform images of pure Gaussian noise into real images, like the handwritten digits of MNIST above, or fancy artwork like the first example from HF's Stable Diffusion pipeline.\n",
        "\n",
        "You can’t help but be amazed by the thought of this incredible transformation from noise to image - it's kind of like \"discovering\" images inside the noise!\n",
        "\n",
        "Let's dig a bit deeper into this process. First, let's imagine that noise is incrementally added to an image (we'll start with a blank image for now) in 1,000 small steps. Every time we take a step, we get a little bit closer to pure Gaussian noise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14cd9c71"
      },
      "outputs": [],
      "source": [
        "steps = 1000\n",
        "fractions = torch.linspace(0, steps-1, 5)/999\n",
        "fractions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61de6da0"
      },
      "source": [
        "We can illustrate this process over a few selected steps: in the beginning, the image is blank, it gets progressively noisier, until it reaches the full level of noise we generated at the beginning. It's like the noise is \"fading in\"!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "794db48c"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(len(fractions), 8, figsize=(10, 5))\n",
        "for i, f in enumerate(fractions):\n",
        "    plot_images(noise*f, axs=axs[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6b5e6bc"
      },
      "source": [
        "Now, let's do the exact opposite with the original MNIST images. We'll \"fade out\" the images over 1,000 steps. The images start as the original ones, and get progressively fainter, until they disappear completely into a blank image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b406967a"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(len(fractions), 8, figsize=(10, 5))\n",
        "for i, f in enumerate(fractions):\n",
        "    plot_images((1-f)*images, axs=axs[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ec94aa1"
      },
      "source": [
        "### Diffusion\n",
        "\n",
        "Now, what happens if we add them up together, the progressively noisier images and fainter digits?\n",
        "\n",
        "That's a simplified diffusion process!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7a6c1e30"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(len(fractions), 8, figsize=(10, 5))\n",
        "for i, f in enumerate(fractions):\n",
        "    plot_images(noise*f+(1-f)*images, axs=axs[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dcaeb79"
      },
      "source": [
        "There is more to this process than meets the eye, though. The diffusion does not have to follow a linear path, as demonstrated in the example above. There are different ways to weigh images and noise, and these are called **schedules**.\n",
        "\n",
        "Unsurprisingly, the objects managing these schedules are called schedulers, marking the first component within the Stable Diffusion pipeline that we'll explore. These schedulers handle the intricate task of both adding and removing noise to and from images, shouldering the heavy lifting for us (more on this later)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "840de59c"
      },
      "source": [
        "### Scheduler\n",
        "\n",
        "The image below, from the “Denoising Diffusion Probabilistic Models” paper by Jonathan Ho, Ajay Jain, and Pieter Abbeel, illustrates both processes:\n",
        "- adding noise to a clean image, from right to left, using `q`\n",
        "- removing noise from a noisy image, from left to right, using `p`\n",
        "\n",
        " Adding noise is relatively straightforward, thanks to some convenient mathematical properties that simplify determining the amount of noise to be added based on the current timestep. While we won’t delve into the details here, you can check Lilian Weng’s insightful blog post, [What are Diffusion Models?](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/) for more specifics.\n",
        "\n",
        "![](https://github.com/dvgodoy/DiffusionModels101_ODSC_Europe2023/blob/main/images/ddpm.png?raw=true)\n",
        "\n",
        "Source: [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239) by Jonathan Ho, Ajay Jain, Pieter Abbeel\n",
        "\n",
        "The `diffusers` library from HuggingFace implements several schedulers, so we can leverage them to seamlessly add noise to our images. First, let's create a scheduler that uses 1,000 timesteps:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10375cac"
      },
      "outputs": [],
      "source": [
        "from diffusers import DDPMScheduler\n",
        "\n",
        "noise_scheduler = DDPMScheduler(num_train_timesteps=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc8d780d"
      },
      "source": [
        "Then, let's evenly divide the timesteps into eight parts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efb6aae7"
      },
      "outputs": [],
      "source": [
        "timesteps = torch.linspace(0, noise_scheduler.config.num_train_timesteps-1, 8).long()\n",
        "timesteps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "047852f7"
      },
      "source": [
        "Next, let's use the eight images we retrieved from our dataset and, for each image, add the noise corresponding to a given timestep using the aptly named `add_noise()` method.\n",
        "\n",
        "It takes three arguments:\n",
        "- clean images\n",
        "- generated noise\n",
        "- timesteps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6d544ba8"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(13)\n",
        "noise = torch.randn_like(images)\n",
        "\n",
        "noisy_images = noise_scheduler.add_noise(images, noise, timesteps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9647c96c"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(3, 8, figsize=(10, 4))\n",
        "plot_images(images, axs=axs[0])\n",
        "plot_images(noisy_images, axs=axs[2])\n",
        "for i, ax in enumerate(axs[1]):\n",
        "    ax.axis('off')\n",
        "    ax.text(.35, .5, str(timesteps[i].item()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9127a7e"
      },
      "source": [
        "As we progress from left to right, there's more and more noise added to the image. Additionally, it  appears that the transition from \"clean\" to \"noisy\" image happens more rapidly than in our previous example. Why is that the case? It’s because the \"fading in\" of the noise and the \"fading out\" of the original image do not follow a linear schedule.\n",
        "\n",
        "Let’s go through *some* mathematical details after all to illustrate the process.\n",
        "\n",
        "The expression below shows us how a given (noisy) image at timestep `t` is a composition of both the original image (`x0`) and pure Gaussian noise (`epsilon`):\n",
        "\n",
        "$$\n",
        "\\Large\n",
        "x_t = {\\sqrt{\\bar{\\alpha}_t}} x_0 + \\sqrt{(1 - \\bar{\\alpha}_t)} \\epsilon\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f060b75"
      },
      "source": [
        "They are weighted by coefficients based on the cumulative product of `alpha`. But what is this `alpha`? They are computed by the scheduler based on the defined schedule. Let's take a look at them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "938bdfa2"
      },
      "outputs": [],
      "source": [
        "noise_scheduler.alphas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8b967281"
      },
      "outputs": [],
      "source": [
        "plt.plot(noise_scheduler.alphas_cumprod ** 0.5, label=r\"${\\sqrt{\\bar{\\alpha}_t}}$\")\n",
        "plt.plot((1 - noise_scheduler.alphas_cumprod) ** 0.5, label=r\"$\\sqrt{(1 - \\bar{\\alpha}_t)}$\")\n",
        "plt.legend(fontsize=\"x-large\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2c3735a"
      },
      "source": [
        "The original image (`x0`) is weighted by the blue line, while the yellow line drives the noise (`epsilon`). In the beginning (`t = 0`), there's only the original image. In the end (`t = 1000`), there's only noise. You may also notice that this schedule is quite different from the naive linear schedule we used to illustrate the diffusion process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79d63b43"
      },
      "source": [
        "### Reverse Diffusion\n",
        "\n",
        "That's where the magic, or better yet, the model, happens! A noisy image at a given timestep is the weighted sum of the original image and the noise:\n",
        "\n",
        "$$\n",
        "\\Large\n",
        "x_t = {\\sqrt{\\bar{\\alpha}_t}} x_0 + \\sqrt{(1 - \\bar{\\alpha}_t)} \\epsilon\n",
        "$$\n",
        "\n",
        "In the diffusion process, we use two variables (the original image and the noise we create) to obtain a third variable (the noisy image).\n",
        "\n",
        "In the reverse diffusion process, we have one variable (the noisy image) and we'd like to obtain another (the clean image)\n",
        "\n",
        "$$\n",
        "\\Large\n",
        "\\hat{x_0} = \\frac{\\left( x_t - \\sqrt{(1 - \\bar{\\alpha}_t)} \\color{red}{\\hat{\\epsilon}} \\right)}{\\sqrt{\\bar{\\alpha}_t}}\n",
        "$$\n",
        "\n",
        "But we're still missing the third variable: the noise (in red). In order to generate a clean image, we need to know the noise, but how?\n",
        "\n",
        "What if we build a model to predict the noise?\n",
        "\n",
        "$$\n",
        "\\Large\n",
        "\\hat{x_0} = \\frac{\\left( x_t - \\sqrt{(1 - \\bar{\\alpha}_t)} \\color{red}{\\text{model}(x, t)} \\right)}{\\sqrt{\\bar{\\alpha}_t}}\n",
        "$$\n",
        "\n",
        "Easy enough, right? Well, in theory, yes. In practice, the model won't be *that* good to predict the right amount of noise in one shot! So, it is actually done **incrementally**: we move one step at a time, from the noisy image towards the clean image using a weighted sum of the noisy image (`xt`) and the **predicted** clean image (`^x0`).\n",
        "\n",
        "$$\n",
        "\\huge\n",
        "\\begin{array}\n",
        "&x_{t-1} &=& c_0 &\\hat{x_0} &+& c_1 &x_t\n",
        "\\\\\n",
        "& =& \\frac{\\sqrt{\\bar{\\alpha}_{t-1}}(1-\\alpha_t)}{1-\\bar{\\alpha}_t}&\\hat{x_0}&+&\\frac{\\sqrt{\\alpha_t}(1-\\bar{\\alpha}_{t-1})}{1-\\bar{\\alpha}_t} &x_t\n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "Both coefficients are based on the alpha variable that drives the schedule, but we won't be going into any further details here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d3c4bfa"
      },
      "source": [
        "#### Cheating\n",
        "\n",
        "Now, let’s take a slight shortcut and construct a “model” that impeccably predicts the noise being added to the images. This approach is considered a bit of a cheat, as we use this “model” both as a noise generator AND a predictor!\n",
        "\n",
        "First, we use it to generate some noise, and feed the noise to the scheduler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9566ec47"
      },
      "outputs": [],
      "source": [
        "def model(x, t):\n",
        "    torch.manual_seed(13)\n",
        "    noise = torch.randn_like(x)\n",
        "    return noise\n",
        "\n",
        "noise = model(images, None)\n",
        "sample = noise_scheduler.add_noise(images, noise, torch.ones(8).long()*999)\n",
        "plot_images(sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cacc3e74"
      },
      "source": [
        "These are noisy MNIST images and they are unrecognizable.\n",
        "\n",
        "Then, let's use our \"model\" to predict epsilon, and feed it to the scheduler's `step()` method, which also takes three arguments:\n",
        "- predicted noise\n",
        "- timesteps\n",
        "- noisy images\n",
        "\n",
        "After calling the `step` method, we can either retrieve the noisy image at the previous (`t-1`) step using the `prev_sample` attribute or the predicted original (clean) image using the `pred_original_sample` attribute.\n",
        "\n",
        "We *know* our model is perfect, so let's take the predicted original sample right away:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cd29050c"
      },
      "outputs": [],
      "source": [
        "t = 999\n",
        "epsilon = model(sample, t)\n",
        "pred_x0 = noise_scheduler.step(epsilon, t, sample).pred_original_sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcf48238"
      },
      "outputs": [],
      "source": [
        "plot_images(pred_x0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "036857da"
      },
      "source": [
        "Perfect digits! The noise was completely removed, as expected, since we're \"cheating\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1d04ec3"
      },
      "source": [
        "#### Less Noise in the Previous Step\n",
        "\n",
        "Of course, perfect models do not exist. In reality, we would be iteratively generating better and better samples as we move backwards in time, using the (hopefully) less noisier sample predicted for `t-1` as input for the next step, until we reach `t = 0`.\n",
        "\n",
        "In code, it looks like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f69f2249"
      },
      "outputs": [],
      "source": [
        "for i, t in enumerate(noise_scheduler.timesteps):\n",
        "    with torch.no_grad():\n",
        "        epsilon = model(sample, t)\n",
        "\n",
        "    sample = noise_scheduler.step(epsilon, t, sample).prev_sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "097504fb"
      },
      "source": [
        "It is time for a real model now, and the typical model used with diffusion processes is the UNet model. In our next post in this series, we’ll explore the diffusers library UNet model class and break down examples showcasing the model’s efficiency.\n",
        "\n",
        "We included a bonus section below, highlighting a few more impressive use cases of diffusion models. We will share content like this throughout the series, so be sure to check eviltux.com for more in-depth guides on diffusion models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "312b6822"
      },
      "source": [
        "## Bonus\n",
        "\n",
        "Generating images out of pure noise is incredible, but it is only the tip of the iceberg! Here we present several other amazing uses of the stable diffusion models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19faf43f"
      },
      "source": [
        "### Image2Image\n",
        "\n",
        "As the name says, it starts with an image, and turns it into another. You can think of it as giving the model a helping hand, so it doesn't have to start from pure noise.\n",
        "\n",
        "The example below comes from a [fast.ai notebook on diffusion](https://github.com/fastai/diffusion-nbs/blob/master/stable_diffusion.ipynb): it takes a rough sketch of a wolf howling at the moon as starting point, and turns it into a really nice image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "887236dc"
      },
      "outputs": [],
      "source": [
        "from diffusers import StableDiffusionImg2ImgPipeline\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", revision=\"fp16\", torch_dtype=torch.float16)\n",
        "pipe.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cfbdbf6"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import requests\n",
        "\n",
        "url = 'https://cdn-uploads.huggingface.co/production/uploads/1664665907257-noauth.png'\n",
        "response = requests.get(url, stream=True)\n",
        "with open('img.png', 'wb') as out_file:\n",
        "    shutil.copyfileobj(response.raw, out_file)\n",
        "\n",
        "from PIL import Image\n",
        "init_image = Image.open('img.png').convert(\"RGB\")\n",
        "init_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52807849"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1000)\n",
        "prompt = \"Wolf howling at the moon, photorealistic 4K\"\n",
        "images = pipe(prompt=prompt, num_images_per_prompt=3, image=init_image, strength=0.8, num_inference_steps=50).images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43a4033e"
      },
      "outputs": [],
      "source": [
        "init_image = images[2]\n",
        "init_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8a25a96f"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1000)\n",
        "prompt = \"Oil painting of wolf howling at the moon by Van Gogh\"\n",
        "new_images = pipe(prompt=prompt, num_images_per_prompt=3, image=init_image, strength=1, num_inference_steps=70).images\n",
        "new_images[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29bf51b6"
      },
      "source": [
        "### Textual Inversion\n",
        "\n",
        "Textual inversion, proposed in the [An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion](https://textual-inversion.github.io/) paper and [repo](https://github.com/rinongal/textual_inversion), is a technique that allows you to consistently place yourself, or anything you want, in the generated images. How would you condition a model to generate images of you? Unless you're famous enough to be part of the training set used to train a CLIP model, chances are you're unknown to CLIP.\n",
        "\n",
        "But, even if you don't want to place yourself, but your dog, the diffusion process will generate \"a\" dog, not \"your\" dog. The generated image may get the breed correct, but it is only a general depiction of the dog breed.\n",
        "\n",
        "The underlying issue is that both you and your dog are unfamiliar to CLIP.. Textual inversion fixes that! How? It takes a specific, rarely used token (for whatever reason \"sks\" is a popular choice), and overfits it to a selection of images of yourself, or your dog. That way, CLIP gets to know you, and you can start calling yourself \"sks\" for the purpose of image generation.\n",
        "\n",
        "You can also check HuggingFace's [textual inversion fine-tuning example](https://github.com/huggingface/diffusers/tree/main/examples/textual_inversion), but the example below comes from a [fast.ai notebook on diffusion](https://github.com/fastai/diffusion-nbs/blob/master/stable_diffusion.ipynb):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e75ba256"
      },
      "outputs": [],
      "source": [
        "pipe = DiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", revision=\"fp16\", torch_dtype=torch.float16)\n",
        "pipe = pipe.to(device)\n",
        "\n",
        "embeds_url = \"https://huggingface.co/sd-concepts-library/indian-watercolor-portraits/resolve/main/learned_embeds.bin\"\n",
        "response = requests.get(embeds_url, stream=True)\n",
        "with open('learned_embeds.bin', 'wb') as out_file:\n",
        "    shutil.copyfileobj(response.raw, out_file)\n",
        "\n",
        "embeds_dict = torch.load('learned_embeds.bin', map_location=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59dffb7e"
      },
      "outputs": [],
      "source": [
        "tokenizer = pipe.tokenizer\n",
        "text_encoder = pipe.text_encoder\n",
        "new_token, embeds = next(iter(embeds_dict.items()))\n",
        "embeds = embeds.to(text_encoder.dtype)\n",
        "new_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5fda424"
      },
      "outputs": [],
      "source": [
        "assert tokenizer.add_tokens(new_token) == 1, \"The token already exists!\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cb3a62b5"
      },
      "outputs": [],
      "source": [
        "text_encoder.resize_token_embeddings(len(tokenizer))\n",
        "new_token_id = tokenizer.convert_tokens_to_ids(new_token)\n",
        "text_encoder.get_input_embeddings().weight.data[new_token_id] = embeds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a025fefe"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1000)\n",
        "image = pipe(\"Woman reading in the style of <watercolor-portrait>\").images[0]\n",
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f954bd1"
      },
      "source": [
        "### DreamBooth\n",
        "\n",
        "DreamBooth is named after the idea of having a photobooth where you enter your dreams, placing yourself (or your dog) anywhere you want using image generation. It is the evolution of the textual inversion idea, but fine-tuning the whole model instead of only the textual embeddings. Before, you made CLIP know you, now, you'll make the whole stable diffusion pipeline aware of your existence :-)\n",
        "\n",
        "Check the official [DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation](https://dreambooth.github.io/) for more details, and HuggingFace's [notebook from the Diffusers course](https://colab.research.google.com/github/huggingface/diffusion-models-class/blob/main/hackathon/dreambooth.ipynb) for a working example."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}