{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Evil-Tux/Diffusion-Models/blob/main/Article_3_Fine_Tune_Pretrained.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VaACDB6dOkCV"
      },
      "outputs": [],
      "source": [
        "!pip install diffusers==0.16.1 accelerate open_clip_torch transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHBOr6azOz45"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchvision\n",
        "from torchvision.transforms import Compose, Resize, ToTensor, ToPILImage\n",
        "\n",
        "from diffusers import DDPMScheduler, DDIMScheduler, DDPMPipeline, DDIMPipeline\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "def plot_images(images, n=8, axs=None):\n",
        "    if axs is None:\n",
        "        fig, axs = plt.subplots(1, n, figsize=(10, 3))\n",
        "    assert len(axs) == len(images)\n",
        "    for i, img in enumerate(images):\n",
        "        axs[i].axis('off')\n",
        "        if isinstance(img, torch.Tensor):\n",
        "            img = ToPILImage()((img/2+0.5).clamp(0, 1))\n",
        "        axs[i].imshow(img.resize((64, 64), resample=Image.NEAREST), cmap='gray_r', vmin=0, vmax=255)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "027286cf"
      },
      "source": [
        "## Fine-Tuning Pretrained Models\n",
        "\n",
        "It is the 2020’s and barely anyone trains deep learning models from scratch anymore. Everyone fine-tunes an existing pretrained model so it fits their own purposes instead. By fine-tuning pretrained models, individuals and companies alike expend fewer resources to produce a usable model which suits their needs. There are many pretrained models to choose from in the [Hugging Face Model Hub](https://huggingface.co/models) with different weights and architectures. In this post, we’re going to fine-tune a pretrained model and upload it to Hugging Face.\n",
        "\n",
        "Let's take a pretrained pipeline, Google's `ddpm-cifar10-32`, a diffusion model trained on the CIFAR10 dataset, and fine-tune it on our own MNIST digits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "912dbf78"
      },
      "source": [
        "### Loading Pretrained Pipeline\n",
        "\n",
        "Let's load the pretrained pipeline directly from the HuggingFace Hub:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2e77f7e"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "image_pipe = DDPMPipeline.from_pretrained(\"google/ddpm-cifar10-32\")\n",
        "image_pipe.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9e10a8e9"
      },
      "outputs": [],
      "source": [
        "image_pipe.unet.num_parameters()/1e6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a49fda04"
      },
      "source": [
        "It’s now time to generate some images, so we know what we're dealing with here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "835b77ed"
      },
      "outputs": [],
      "source": [
        "images = image_pipe(batch_size=8).images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ef5b20b"
      },
      "source": [
        "Although you can use the provided code, it’s worth reconsidering this approach. Why? Generating images with DDPM proves to be a time-intensive process.\n",
        "\n",
        "Instead, let's create a much faster DDIM scheduler and assign it to the pipeline:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f23d8da8"
      },
      "outputs": [],
      "source": [
        "scheduler = DDIMScheduler.from_pretrained('google/ddpm-cifar10-32')\n",
        "image_pipe.scheduler = scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4baf9bc6"
      },
      "outputs": [],
      "source": [
        "images = image_pipe(batch_size=8, num_inference_steps=40).images"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Much better! Let's take a look at the images."
      ],
      "metadata": {
        "id": "wJlufH2iHCKj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71e9f3b4"
      },
      "outputs": [],
      "source": [
        "plot_images(images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcf9f0ae"
      },
      "source": [
        "They are indeed CIFAR-ish, as expected. Notice that, unlike MNIST images, they are colored RGB images. Therefore, we need to adjust our own images to the pipeline's expected input: 3-channel 32x32 pixel images. We can use Torchvision's Lambda transform to replicate MNIST's single channel three times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "af7f74dc"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms import Lambda\n",
        "\n",
        "composed = Compose([Resize(32), ToTensor(), Lambda(lambda x: x.repeat(3, 1, 1))])\n",
        "dataset = torchvision.datasets.MNIST(root=\"mnist/\", train=True, download=True, transform=composed)\n",
        "train_dataloader = DataLoader(dataset, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "848b65a4"
      },
      "outputs": [],
      "source": [
        "images = next(iter(train_dataloader))[0][:8]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6e1ab1df"
      },
      "outputs": [],
      "source": [
        "plot_images(images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16774133"
      },
      "source": [
        "### Training Loop\n",
        "\n",
        "The training loop is essentially the same as the one we used to train a model from scratch. There are a few small differences, since we're using both scheduler and model directly from the pipeline, `pipeline.scheduler` and `pipeline.unet`, respectively.\n",
        "\n",
        "It goes over the same steps (1-6; see below) and typical PyTorch training stuff (computing gradients, updating parameters, zeroing gradients).\n",
        "\n",
        "This code was adapted from [Unit 2 of HuggingFace's Diffusion Models class](https://colab.research.google.com/github/huggingface/diffusion-models-class/blob/main/unit2/01_finetuning_and_guidance.ipynb). Let's take a look at the output below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21101a47"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Sending pipeline to device\n",
        "image_pipe.to(device)\n",
        "\n",
        "## FIXED A MISTAKED HERE - PLEASE UPDATE ##\n",
        "optimizer = torch.optim.AdamW(image_pipe.unet.parameters(), lr=1e-5)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "# Fetching scheduler from the pipeline\n",
        "num_train_timesteps = image_pipe.scheduler.config.num_train_timesteps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dd10d1b"
      },
      "outputs": [],
      "source": [
        "losses = []\n",
        "\n",
        "for epoch in tqdm(range(3)):\n",
        "    for step, batch in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
        "        # Step 1: Fetch Clean Images\n",
        "        clean_images = batch[0].to(device)\n",
        "\n",
        "        # Step 2: Generate (Full) Noise\n",
        "        noise = torch.randn_like(clean_images).to(device)\n",
        "\n",
        "        # Step 3: Random Timesteps\n",
        "        bs = clean_images.shape[0]\n",
        "        t = torch.randint(0, num_train_timesteps, (bs,), device=device).long()\n",
        "\n",
        "        # Step 4: Add Noise to Clean Images\n",
        "        # Fetching scheduler from the pipeline\n",
        "        noisy_images = image_pipe.scheduler.add_noise(clean_images, noise, t)\n",
        "\n",
        "        # Step 5: Predict (Full) Noise from (Partially) Noisy Images\n",
        "        # Fetching model from the pipeline\n",
        "        noise_pred = image_pipe.unet(noisy_images, t, return_dict=False)[0]\n",
        "\n",
        "        # Step 6: Compute Loss\n",
        "        loss = loss_fn(noise_pred, noise)\n",
        "\n",
        "        # Regular PyTorch training loop stuff\n",
        "        loss.backward(loss)\n",
        "        losses.append(loss.item())\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    loss_last_epoch = sum(losses[-len(train_dataloader) :]) / len(train_dataloader)\n",
        "    print(f\"Epoch:{epoch+1}, loss: {loss_last_epoch}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9f25e238"
      },
      "outputs": [],
      "source": [
        "plt.plot(losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da5fb09b"
      },
      "source": [
        "Given the size of the model, it may take  around 20 minutes to fine-tune it on Google Colab. Instead, we'll be loading the fine-tuned model in a couple of sections.\n",
        "\n",
        "But, if you're running it and waiting for it to finish, you should expect to see losses like this:\n",
        "\n",
        "![](https://github.com/dvgodoy/DiffusionModels101_ODSC_Europe2023/blob/main/images/diffusion_finetuning_mnist_losses.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3acceb0b"
      },
      "source": [
        "### Pushing to Hub\n",
        "\n",
        "You don't need to run these cells - I've kept them here so you can see how to push a fine-tuned pipeline to HuggingFace's Hub, in case you'd like to share it with others."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3850f1d5"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2c783bb"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import HfApi, create_repo\n",
        "\n",
        "hub_model_id = \"dvgodoy/ddpm-cifar10-32-mnist\"\n",
        "create_repo(hub_model_id)\n",
        "api = HfApi()\n",
        "api.upload_folder(\n",
        "    folder_path=\"ddpm-cifar10-32-mnist/scheduler\", path_in_repo=\"\", repo_id=hub_model_id\n",
        ")\n",
        "api.upload_folder(folder_path=\"ddpm-cifar10-32-mnist/unet\", path_in_repo=\"\", repo_id=hub_model_id)\n",
        "api.upload_file(\n",
        "    path_or_fileobj=\"ddpm-cifar10-32-mnist/model_index.json\",\n",
        "    path_in_repo=\"model_index.json\",\n",
        "    repo_id=hub_model_id,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3c90c11"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import ModelCard\n",
        "\n",
        "content = f\"\"\"\n",
        "---\n",
        "license: mit\n",
        "tags:\n",
        "- pytorch\n",
        "- diffusers\n",
        "- unconditional-image-generation\n",
        "- diffusion-models-class\n",
        "---\n",
        "\n",
        "# Diffusion Models 101\n",
        "\n",
        "This model is a diffusion model for unconditional image generation of MNIST digits fine-tuned on Google's ddpm-cifar10-32 model\n",
        "\n",
        "## Usage\n",
        "\n",
        "```python\n",
        "from diffusers import DDPMPipeline\n",
        "\n",
        "pipeline = DDPMPipeline.from_pretrained('{hub_model_id}')\n",
        "image = pipeline().images[0]\n",
        "image\n",
        "```\n",
        "\"\"\"\n",
        "\n",
        "card = ModelCard(content)\n",
        "card.push_to_hub(hub_model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91e7df31"
      },
      "source": [
        "### Loading From Hub\n",
        "\n",
        "Once the model is pushed to the Hub, you can load it like any other model from there.\n",
        "\n",
        "To save you time, let's just load the resulting model and pipeline instead of running the training loop above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45010410"
      },
      "outputs": [],
      "source": [
        "from diffusers import DDPMPipeline, DDIMScheduler\n",
        "image_pipe = DDPMPipeline.from_pretrained('dvgodoy/ddpm-cifar10-32-mnist')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb8563c6"
      },
      "source": [
        "Then we can generate images in the usual way:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60cd65e0"
      },
      "outputs": [],
      "source": [
        "image_pipe.to(device)\n",
        "images = image_pipe(batch_size=8, num_inference_steps=40).images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4b0d6df3"
      },
      "outputs": [],
      "source": [
        "plot_images(images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a3901ce"
      },
      "source": [
        "On the up side, these images are not CIFAR-ish at all! On the down side, they do not quite look like MNIST digits yet...\n",
        "\n",
        "Well, it actually depends on whom you ask :-)\n",
        "\n",
        "![](https://github.com/dvgodoy/DiffusionModels101_ODSC_Europe2023/blob/main/images/paracetamol.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7fd8229"
      },
      "source": [
        "### Generating Images\n",
        "\n",
        "Please bear with me as we bring back the manual loop for image generation. You'll see why we're doing this shortly. Just like in the training loop, we're taking both scheduler and model directly from the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9edeb5e4"
      },
      "outputs": [],
      "source": [
        "noise_scheduler = image_pipe.scheduler\n",
        "model = image_pipe.unet\n",
        "\n",
        "torch.manual_seed(33)\n",
        "sample = torch.randn(8, 3, 32, 32).to(device)\n",
        "\n",
        "for i, t in tqdm(enumerate(noise_scheduler.timesteps)):\n",
        "    # Ensures schedulers are interchangeable\n",
        "    model_input = noise_scheduler.scale_model_input(sample, t)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        epsilon = model(sample, t).sample\n",
        "\n",
        "    sample = noise_scheduler.step(epsilon, t, sample).prev_sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c757d3b"
      },
      "source": [
        "Then, let's generate some images as sanity check:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3c73a0b"
      },
      "outputs": [],
      "source": [
        "plot_images(sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b9786bb"
      },
      "source": [
        "Cool, they still look like handwritten digits from a doctor :-)\n",
        "\n",
        "Or, perhaps, they look like falling characters from \"The Matrix\"?\n",
        "\n",
        "We loaded a pretrained model and fine-tuned it to generate MNIST digit images, however, we can further adjust the model. In the next post in the Diffusion Model series, we’ll learn how to guide the model to generate images with a specific characteristic."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}