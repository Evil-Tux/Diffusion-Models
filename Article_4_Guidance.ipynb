{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Evil-Tux/Diffusion-Models/blob/main/Article_4_Guidance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VaACDB6dOkCV"
      },
      "outputs": [],
      "source": [
        "!pip install diffusers==0.16.1 accelerate open_clip_torch transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHBOr6azOz45"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchvision\n",
        "from torchvision.transforms import Compose, Resize, ToTensor, ToPILImage\n",
        "\n",
        "from diffusers import DDPMScheduler, DDIMScheduler, DDPMPipeline, DDIMPipeline\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "def plot_images(images, n=8, axs=None):\n",
        "    if axs is None:\n",
        "        fig, axs = plt.subplots(1, n, figsize=(10, 3))\n",
        "    assert len(axs) == len(images)\n",
        "    for i, img in enumerate(images):\n",
        "        axs[i].axis('off')\n",
        "        if isinstance(img, torch.Tensor):\n",
        "            img = ToPILImage()((img/2+0.5).clamp(0, 1))\n",
        "        axs[i].imshow(img.resize((64, 64), resample=Image.NEAREST), cmap='gray_r', vmin=0, vmax=255)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7a6ee9d"
      },
      "source": [
        "## Guidance\n",
        "\n",
        "In the previous post in the Diffusion Models 101 series, we fine-tuned a pretrained model to generate MNIST digit images. When we generated images using the manual loop as opposed to the training loop, the generated digits resembled characters from “The Matrix”.\n",
        "\n",
        "What if we wanted our digits to be green as the falling characters from \"The Matrix\"? They kind of look like that already, so why not?\n",
        "\n",
        "The question is, how can we tell the model we like our digits green?\n",
        "\n",
        "We need to guide our model into the right path or, as Morpheus said to Neo, \"there's a difference between knowing the path, and walking the path.\"\n",
        "\n",
        "Deep, right?\n",
        "\n",
        "With guidance, we can manipulate the quality of the output images by directing the generative process towards a specified outcome. In this post, we will demonstrate the flexibility of diffusion models and target color modification of the generated image output using guidance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_uqetTIcWik"
      },
      "outputs": [],
      "source": [
        "#########\n",
        "## NEW ##\n",
        "#########\n",
        "from diffusers import DDPMPipeline, DDIMScheduler\n",
        "image_pipe = DDPMPipeline.from_pretrained('dvgodoy/ddpm-cifar10-32-mnist')\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "image_pipe.to(device)\n",
        "noise_scheduler = image_pipe.scheduler\n",
        "noise_scheduler.set_timesteps(40)\n",
        "model = image_pipe.unet\n",
        "\n",
        "torch.manual_seed(33)\n",
        "sample = torch.randn(8, 3, 32, 32).to(device)\n",
        "\n",
        "for i, t in tqdm(enumerate(noise_scheduler.timesteps)):\n",
        "    # Ensures schedulers are interchangeable\n",
        "    model_input = noise_scheduler.scale_model_input(sample, t)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        epsilon = model(sample, t).sample\n",
        "\n",
        "    sample = noise_scheduler.step(epsilon, t, sample).prev_sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19Muk7BZc3uP"
      },
      "outputs": [],
      "source": [
        "#########\n",
        "## NEW ##\n",
        "#########\n",
        "plot_images(sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd69f7ae"
      },
      "source": [
        "### Color\n",
        "\n",
        "Let’s start by defining the color of \"The Matrix\", and generating a single pixel colored like that, expanded to the expected four dimensions of a PyTorch mini-batch of images: the number of data samples, image channels, image height, and image width (NCHW)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "722c081f",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# The Matrix\n",
        "color = (28/255, 161/255, 82/255)\n",
        "\n",
        "colored_pixel = torch.tensor(color)[None, :, None, None]\n",
        "colored_pixel, colored_pixel.shape # NCHW\n",
        "ToPILImage()(colored_pixel[0]).resize((64, 64))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37445bc8"
      },
      "source": [
        "That's green for sure! Now, let's center its values at zero:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69d92573"
      },
      "outputs": [],
      "source": [
        "colored_pixel = colored_pixel * 2 - 1\n",
        "colored_pixel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1a4fec0"
      },
      "source": [
        "#### Color Loss\n",
        "\n",
        "Once our guiding pixel is on the same footing as our noisy samples, we can take the mean absolute difference between them. We only have ONE green pixel to compare the other image to, but broadcasting has our backs, so it will effectively compare that lone pixel to every other pixel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5437cc75"
      },
      "outputs": [],
      "source": [
        "# Mean pixel difference between an image and our (broadcast) colored pixel\n",
        "torch.abs(sample[0] - colored_pixel.to(device)).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe364f03"
      },
      "source": [
        "Let's organize this code into a function that takes a mini-batch of images and returns the corresponding loss:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5be62cc"
      },
      "outputs": [],
      "source": [
        "def color_loss(images, color):\n",
        "    colored_pixel = torch.tensor(color).to(images.device)[None, :, None, None]\n",
        "    colored_pixel = colored_pixel * 2 - 1\n",
        "    errors = torch.abs(images - colored_pixel)\n",
        "    loss = errors.mean()\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "239bb3e2"
      },
      "outputs": [],
      "source": [
        "color_loss(sample, color)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1a3cccc"
      },
      "source": [
        "That's a loss value, but aren't you **missing** something?\n",
        "\n",
        "We usually call the `backward()` method on losses, but it only makes sense to do so if the loss itself is gradient-requiring tensor, right? Let's fix that by **making our samples require gradient**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0200287"
      },
      "outputs": [],
      "source": [
        "sample_with_grad = sample.detach().requires_grad_()\n",
        "loss = color_loss(sample_with_grad, color)\n",
        "loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75ae1f09"
      },
      "source": [
        "Just like we update parameters using their gradients (based on the MSE loss, for example) and a learning rate, we'll update samples using their gradients based on the color loss we used as guidance and a learning rate-equivalent called `guidance_loss_scale`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86db856d"
      },
      "outputs": [],
      "source": [
        "guidance_loss_scale = 40\n",
        "\n",
        "grad = torch.autograd.grad(loss, sample_with_grad)[0]\n",
        "\n",
        "sample = sample_with_grad.detach()\n",
        "sample = sample - guidance_loss_scale * grad  # analogous to w = w - lr * grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fe92d4d"
      },
      "source": [
        "Notice that we're detaching the sample (i.e., removing the gradient requirement), because it will be an input to the scheduler's `step()` method.\n",
        "\n",
        "Moreover, we will be computing the loss by comparing the **predicted denoised image**, the image we're actually interested in, and the one that we'd like to be green, to the guiding green pixel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04e6ccd6"
      },
      "source": [
        "### Generating Images\n",
        "\n",
        "Let's update our generation loop to include all the steps from the guidance code:\n",
        "1. Make samples require gradient\n",
        "2. Compute the color loss between the predicted denoised image and the guiding color\n",
        "3. Compute gradients of the color loss with reference to the image's pixels\n",
        "4. Detach the samples and update them using the gradients and the guidance loss scale (the \"learning rate\")\n",
        "\n",
        "Other than that, the following code also includes a call to the scheduler's `scale_model_input()` method that ensures the schedulers are interchangeable (this was copied from the pipeline's generation method, but it has no impact in this example)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dd049782"
      },
      "outputs": [],
      "source": [
        "noise_scheduler = image_pipe.scheduler\n",
        "model = image_pipe.unet\n",
        "\n",
        "torch.manual_seed(33)\n",
        "sample = torch.randn(8, 3, 32, 32).to(device)\n",
        "guidance_loss_scale = 40\n",
        "\n",
        "for i, t in tqdm(enumerate(noise_scheduler.timesteps)):\n",
        "    # Ensures schedulers are interchangeable\n",
        "    model_input = noise_scheduler.scale_model_input(sample, t)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        epsilon = model(sample, t).sample\n",
        "\n",
        "    ## GUIDANCE ##\n",
        "    # Step 1\n",
        "    sample_with_grad = sample.detach().requires_grad_()\n",
        "\n",
        "    # Step 2\n",
        "    # What does the denoised image look like at this point?\n",
        "    pred_x0 = noise_scheduler.step(epsilon, t, sample_with_grad).pred_original_sample\n",
        "    # Does it have the right color?\n",
        "    loss = color_loss(pred_x0, color)\n",
        "    if i % 10 == 0:\n",
        "        print(i, \"loss:\", loss.item())\n",
        "\n",
        "    # Step 3\n",
        "    # Compute gradient\n",
        "    grad = torch.autograd.grad(loss, sample_with_grad)[0]\n",
        "\n",
        "    # Step 4\n",
        "    # Detach the sample so it is a regular tensor again\n",
        "    sample = sample_with_grad.detach()\n",
        "    # Update the sample\n",
        "    sample = sample - guidance_loss_scale * grad\n",
        "    ##############\n",
        "\n",
        "    # Uses the updated sample in the next step\n",
        "    sample = noise_scheduler.step(epsilon, t, sample).prev_sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f5a7d70"
      },
      "source": [
        "What do the resulting images look like?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4b98fa4"
      },
      "outputs": [],
      "source": [
        "plot_images(sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00f16a81"
      },
      "source": [
        "WOW! They are definitely green like \"The Matrix\", so we guided them well into being green. But they apparently got lost in the digits department. The generation process became biased towards complying with the guidance, at the expense of the underlying task: generating digits.\n",
        "\n",
        "You can try adjusting the `guidance_loss_scale` variable to see if you can make them more digit-ish and less green.\n",
        "\n",
        "But there's also a different way of incorporating the guidance into the generation loop: gradients all the way!\n",
        "\n",
        "We're moving Step 1 to the very top, making samples gradient-requiring from the get go, and ditching the `no_grad()` context manager altogether.\n",
        "\n",
        "Notice that the loss is computed using `pred_x0`, which is computed using the sample that is requiring gradients now. This means that the denoising process itself is part of the dynamic computation graph now, thus affecting the gradients used to update the sample in the guidance process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3e6061da"
      },
      "outputs": [],
      "source": [
        "noise_scheduler = image_pipe.scheduler\n",
        "model = image_pipe.unet\n",
        "\n",
        "torch.manual_seed(33)\n",
        "sample = torch.randn(8, 3, 32, 32).to(device)\n",
        "guidance_loss_scale = 150\n",
        "\n",
        "for i, t in tqdm(enumerate(noise_scheduler.timesteps)):\n",
        "    # Step 1\n",
        "    sample_with_grad = sample.detach().requires_grad_()\n",
        "\n",
        "    # Ensures schedulers are interchangeable\n",
        "    model_input = noise_scheduler.scale_model_input(sample_with_grad, t)\n",
        "\n",
        "    # with torch.no_grad():\n",
        "    epsilon = model(sample_with_grad, t).sample\n",
        "\n",
        "    ## GUIDANCE ##\n",
        "    # Step 2\n",
        "    # What does the denoised image look like at this point?\n",
        "    pred_x0 = noise_scheduler.step(epsilon, t, sample_with_grad).pred_original_sample\n",
        "\n",
        "    # Does it have the right color?\n",
        "    loss = color_loss(pred_x0, color)\n",
        "    if i % 10 == 0:\n",
        "        print(i, \"loss:\", loss.item())\n",
        "\n",
        "    # Step 3\n",
        "    # Compute gradient\n",
        "    grad = torch.autograd.grad(loss, sample_with_grad)[0]\n",
        "\n",
        "    # Step 4\n",
        "    # Detach the sample so it is a regular tensor again\n",
        "    sample = sample_with_grad.detach()\n",
        "    # Update the sample\n",
        "    sample = sample - guidance_loss_scale * grad\n",
        "\n",
        "    # Uses the updated sample in the next step\n",
        "    sample = noise_scheduler.step(epsilon, t, sample).prev_sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c000992"
      },
      "source": [
        "Let's take a look at the images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cf9386b"
      },
      "outputs": [],
      "source": [
        "plot_images(sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15580f5a"
      },
      "source": [
        "They are more digit-ish now, that's for sure, and some of them are green-ish. Again, you can try tweaking the `guidance_loss_scale` to try generating more-balanced images.\n",
        "\n",
        "But let's be honest, turning images green isn't that impressive, and it's also quite some work since we had to define a custom loss function just for that.\n",
        "\n",
        "What if we could use words to guide our model instead?\n",
        "\n",
        "In our next post in this series, we’ll take guidance to the next level and use Contrastive Language-Image Pre-training (CLIP) to direct our model to generate digits we specify through a text prompt. Stay tuned!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}